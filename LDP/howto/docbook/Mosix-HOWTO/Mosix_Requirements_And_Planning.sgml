<CHAPTER ID="Requirementsplanning">
<TITLE>Requirements and Planning</TITLE>

<SECT1> <TITLE>Hardware requirements</TITLE> 
<PARA> Installing a basic
clusters requires at least 2 machines with network connected.  Either
using a crosscable between the two network cards or a switch or hub.  Off
course the faster your networkcards the easier you will get better
performance for your cluster.  These days Fast Ethernet is standard,
putting multiple ports in a machine isn`t that difficult, but make sure to
connect them through other physical networks in order to gain the speed
you want.  Gigabit ethernet is getting cheaper any day now but I suggest
that you don`t rush to the shop spending your money before you have
actually tested your setup with multiple 100Mbit cards and noticed that
you really do need the extra network capacity.  
</PARA> </SECT1>

<SECT1>
<TITLE>Hardware Setup Guidelines</TITLE>

<PARA> 
Setting up a big cluster requires some thinking to be done, where
are you going to put the machines, not under a table somehwere or in the
middle of your office.  It`s ok if you just want to do some small tests ,
but if you are planning to deploy a N node cluster you will have to make
sure that the environment that will hold this machine is capable of doing
so.  I`m talking about preparing one or more 19" racks to host the
machines, configure the appropirate network topology, either straight,
single connected or even a 1 to 1 cross connected network between al your
nodes.  You will also need to make sure that there is enough power to
support such a range of machines.  That your airconditioning system
supports the load and that in case of powerfailure your UPS can cleanly
shut down al the required systems. You might want to invest in a KVM
Switch in order to fasciliate access to the machines consoles.  But even
if you don`t have the number of nodes that justify these investments, make
sure that you can always easily access the different nodes, you never know
when you have to replace the fan or a harddisk of a machine in trouble.  
If that means that you have to unload a stack of machines to reach the
bottom one hence shutting down your cluster you are in trouble.  
</PARA>

</SECT1>

<SECT1>
<TITLE>Software requirements</TITLE>

<PARA> The systems we plan to use will need a basic Linux installation of
your choice, RedHat , Suse , Debian or another distribution, it doesn`t
really matter which one.

What does matter is that the kernel is at least on 2.4 level, and that
your networkcards are configured correctly, next to that you`ll need a
healthy space of swap. </PARA>

</SECT1>


<SECT1>
<TITLE>Planning your cluster</TITLE>
<PARA>

How to configure MOSIX clusters with a pool of servers and a set of (personal) 
workstations: 

<itemizedlist>
<listitem>
<para>
Single-pool = all the servers and workstations are used as a single cluster: 
install the same "mosix.map" in all the computers, with the IP
addresses of all the computers. 

Advantage/disadvantage: your workstation is part of the pool. 
</para>
</listitem>
<listitem>
<para>
Server-pool = servers are shared while workstations are not part of the cluster: 
install the same "mosix.map" in all the servers, with the
IP addresses of only the servers. 

Advantage/disadvantage: remote processes will not move to your workstation. You 
need to login to one of the servers to use the
cluster. 
</para>
</listitem>
<listitem>
<para>


Adaptive-pool = servers are shared while workstations join or leave the cluster, 
e.g. from 5PM to 8AM: install the same "mosix.map" in
all the computers, with the IP addresses of all the servers and workstations, 
then use a simple script, to decide whether MOSIX should be
activated or deactivated. 

Advantage/disadvantage: remote processes can use your workstation when you are 
not using it. 
</para>
</listitem>
</itemizedlist>
</PARA>
</SECT1>


</CHAPTER>

