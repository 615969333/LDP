<CHAPTER ID="WHAT">
<TITLE>So what is openMosix Anyway ? </TITLE>
<SECT1><TITLE>A very, very brief introduction to clustering </TITLE>
<PARA>


 Most of the time, your computer is bored. Start a program like xload
  or top that monitors your system use, and you will probably find that
  your processor load is not even hitting the 1.0 mark. If you have two
  or more computers, chances are that at any given time, at least one
  of them is doing nothing. Unfortunately, when you really do need CPU
  power - during a C++ compile, or coding Ogg Vorbis music files - you
  need a lot of it at once. The idea behind clustering is to spread
  these loads among all available computers, using the resources that
  are free on other machines.

</PARA>
<PARA>

 The basic unit of a cluster is a single computer, also called a
  "node". Clusters can grow in size - they "scale" - by adding more
  machines. A cluster as a whole will be more powerful the faster the
  individual computers and the faster their connection speeds are. In
  addition, the operating system of the cluster must make the best use
  of the available hardware in response to changing conditions. This
  becomes more of a challenge if the cluster is composed of different
  hardware types (a "heterogeneous" cluster), if the configuration of
  the cluster changes unpredictably (machines joining and leaving the
  cluster), and the loads cannot be predicted ahead of time.  

</PARA>

<SECT2>
<TITLE>A very, very brief introduction to clustering </TITLE>
<SECT3>
<TITLE>HPC vs Fail-over vs Load-balancing</TITLE>
<PARA>
Basically there are 3 types of clusters, the most deployed ones are 
probably the Fail-over Cluster and the Load-balancing Cluster,  HIGH 
Performance Computing.
</PARA><PARA>
Fail-over Clusters consist of 2 or more network 
connected computers with a separate heartbeat connection between the 2 
hosts.   The Heartbeat connection between the 2 machines is being used to 
monitor whether all the services are still in use,  as soon as a service on 
one machine breaks down the other machine tries to take over.
</PARA> 

<PARA>
With load-balancing clusters the concept is that when a request for say a 
web-server comes in,  the cluster checks which machine is the lease busy and 
then sends the request to that machine.  Actually most of the times a 
Load-balancing cluster is also Fail-over cluster but with the extra load 
balancing functionality and often with more nodes.
</PARA> 

<PARA>The last variation of clustering is the High Performance Computing
Cluster, this machine is being configured specially to give data centers
that require extreme performance the performance they need.  Beowulf's have
been developed especially to give research facilities the computing speed
they need. These kind of clusters also have some load-balancing features,
they try to spread different processes to more machines in order to gain 
performance. But what it mainly comes down to in this situation is that a 
process is being parallelized and that routines that can be ran 
separately will be spread on different machines in stead of having to wait 
till they get done one after another.  


</PARA>

</SECT3> 

<SECT3>
<TITLE>Mainframes and supercomputers vs. clusters</TITLE>
<PARA>

Traditionally Mainframes and Supercomputers have only been built by a 
selected number of vendors, a company or organization that required the 
performance of such a machine had to have a huge budget available for 
it`s  Supercomputer.  Lot`s of universities could not afford them the 
costs of a Supercomputer, therefore other alternatives were being 
researched by them.  The concept of a cluster was born when people first 
tried to spread different jobs over more computers and then gather back 
the data those jobs produced.  With cheaper and more common hardware 
available to everybody, results similar to real Supercomputers were only
to be dreamed of during the first years, but as the PC platform developed 
further, the performance gap between a Supercomputer and a cluster of 
multiple personal computers became smaller.  

</PARA> 
</SECT3> 
<SECT3> 
<TITLE>Cluster models [(N)UMA,  PVM/MPI]</TITLE>
<PARA>
There are different ways of doing parallel processing, (N)UMA, DSM , PVM, MPI are all different 
kinds of Parallel processing schemes.
</PARA>
<PARA> 
(N)UMA , (Non-)Uniform Memory Access machines for example 
have shared access to the memory where they can execute their code.  In the Linux kernel there is a
NUMA implementation that varies the memory access times for different regions of memory.  It then is the kernel's 
task to use the memory that is the closest to the CPU it is using.
</PARA>
<PARA>
PVM / MPI  are the tools that are most commonly being used when people talk about GNU/Linux based 
Beowulf's.
MPI stands for Message Passing Interface it is the open  standard specification for message 
passing libraries.  MPICH is one of the most used implementations of MPI, next to MPICH  you also can use LAM
, another implementation of MPI based on the free reference implementation of the libraries.

</PARA>
<PARA>
PVM or Parallel Virtual Machine is another cousin of MPI  that is also quite often being used as a tool to create 
a Beowulf.   PVM lives in user space so no special kernel modifications are required, basically each user with 
enough rights can run PVM.


</PARA>
</SECT3>
<SECT3>
<TITLE>Mosix's role
</TITLE>
<PARA>

  The Mosix software packages turns networked computers running
  GNU/Linux into a cluster. It automatically balances the load between
  different nodes of the cluster, and nodes can join or leave the
  running cluster without disruption. The load is spread out among
  nodes according to their connection and CPU speeds. 

</PARA>
<PARA>

  Since Mosix is part of the kernel and maintains full compatibility
  with normal Linux, a user's programs, files, and other resources will
  all work as before with no changes necessary. The casual user will
  not notice the difference between Linux and Mosix. To him, the whole
  cluster will function as one (fast) GNU/Linux system.

</PARA>
</SECT3>
</SECT2>


</SECT1>
<SECT1><TITLE>The story so far</TITLE>
<SECT2><TITLE>Historical Development</TITLE>
<PARA>
(To Be Written) 

 The name "Mosix" comes from FEHLT. 


   
   The 6th incarnation of Mosix was developed for BSD/OS. 


   GNU/Linux was chosen as a development platform for the 7th
   incarnation in DATE_FEHLT because of 

</PARA>
</SECT2>
<SECT2><TITLE>Current state</TITLE>
<PARA>
(To Be Written) 

  Like most active Open Source programs, Mosix's rate of change tends
   to outstrip the the follower's ability to keep the documentation up
   to date. See the Mosix Home Page for current news. The following
   relates to Mosix VERSION FEHLT for the Linux kernel FEHLT as of
   DATUM FEHLT: 


</PARA>

</SECT2>
<SECT2><TITLE>openMosix</TITLE>
<!-- Contributed by Matt -->


<PARA>
openMosix is in addition to whatever you find at mosix.org and in full 
appreciation and
respect for Prof. Barak's leadership in the outstanding Mosix project . 
</PARA><PARA>
Moshe Bar has been involved for a number of years with the Mosix project 
(www.mosix.com)
and was co-project manager of the Mosix project and general manager of the 
commercial Mosix company.
</PARA><PARA>

After a difference of  opinions on the commercial future of Mosix, he has 
started a new clustering
company - Qlusters, Inc. - and Prof. Barak has decided not to participate 
for the moment in this venture
(although he did seriously consider joining) and held long running 
negotiations with investors. 
It appears that Mosix is not any longer supported openly as a GPL project.
Because there is a significant user base out there (about 1000 
installations world-wide),
Moshe Bar has decided to continue the development and support of the Mosix 
project under a new name,
openMosix under the full GPL2 license. Whatever code in openMosix comes 
from the old Mosix project
is Copyright 2002 by Amnon Bark. All the new code is copyright 2002 by 
Moshe Bar.
</PARA><PARA>

openMosix is a Linux-kernel patch which provides full compatibility with 
standard Linux for IA32-compatible
platforms. The internal load-balancing algorithm transparently migrates 
processes to other cluster members.
The advantage is a better load-sharing between the nodes. The cluster 
itself tries to optimize utilization
at any time (of course the sysadmin can affect these automatic 
load-balancing by manual configuration during
runtime).
</PARA><PARA>

This transparent process-migration feature make the whole cluster look 
like a BIG SMP-system with as many
processors as available cluster-nodes (of course multiplied with 2 for 
dual-processor systems).
openMosix also provides a powerful mized for HPC-applications, which 
unlike NFS provides cache consistency,
time stamp consistency and link consistency. 
</PARA><PARA>

There could (and will) be significant changes in the architecture of the 
future openMosix versions.
New concepts about auto-configuration, node-discovery and new user-land 
tools are discussed in the
openMosix-mailing-list.
</PARA><PARA>

To approach standardization and future compatibility the proc-interface 
changes from /proc/mosix to /proc/hpc
and the /etc/mosix.map was exchanged to /etc/hpc.map.
Adapted command-line user-space tools for openMosix are already available 
on the web-page of the project and
from the current version (1.1) Mosixview supports openMosix as well.
</PARA><PARA>

The hpc.map will be replaced in the future with a node-auto-discovery 
system.
</PARA><PARA>

openMosix is supported by various competent people (see www.openMosix.org) 
working together around the world.
The gain of the project is to create a standardize clustering-environment 
for all kinds of HPC-applications.
</PARA><PARA>

openMosix has also a project web-page at <ulink url="http://openMosix.sourceforge.net"><citetitle>http://openMosix.sourceforge.net</citetitle></ulink> 
with a CVS tree and mailing-list
for the developer and user.










</PARA>

</SECT2>
</SECT1>

<SECT1><TITLE> Mosix in action: An example</TITLE>
<PARA>

 Mosix clusters can take various forms. To demonstrate, let's assume
  you are a student and share a dorm room with a rich computer science
  guy, with whom you have linked computers to form a Mosix cluster.
  Let's also assume you are currently converting music files from your
  Cd's to Ogg Vorbis for your private use, which is legal in your
  country. Your roommate is working on a project in C++ that he says
  will bring World Peace. However, at just this moment he is in the
  bathroom doing unspeakable things, and his computer is idle. 

</PARA><PARA>

  So when you start a program called FEHLT to convert Bach's
  .... from .wav to .ogg format, the Mosix routines on your
  machine compare the load on both nodes and decide that things will go
  faster if that process is sent from your Pentium-233 to his Athlon
  XP. This happens automatically - you just type or click your commands
  as you would if you were on a standalone machine. All you notice is
  that when you start two more coding runs, things go a lot faster, and
  the response time doesn't go down. 

</PARA><PARA>

  Now while you're still typing ...., your roommate
  comes back, mumbling something about red chile peppers in cafeteria
  food. He resumes his tests, using a program called 'pmake', a version
  of 'make' optimized for parallel execution. Whatever he's doing, it
  uses up so much CPU time that Mosix even starts to send subprocesses
  to your machine to balance the load. 

</PARA><PARA>

  This setup is called *single-pool*: All computers are used as a
  single cluster. The advantage/disadvantage of this is that you
  computer is part of the pool: Your stuff will run on other computers,
  but their stuff will run on your's, too. 

</PARA>
</SECT1>
<SECT1><TITLE>Components</TITLE>
<SECT2><TITLE>Process migration</TITLE>
<PARA>

With mosix you can start a process on one machine and find out it actually runs on 
another machine in the cluster.
Each process has it own unique home node (UHN) where it is created. 
</PARA>
<PARA>

Migration means that a process is splitted in 2 parts, a  user part and a system part
The user part will be moved to a remote node where the system part will stay on the UHN 
and.  This system-part is sometimes called the deputy process, this process takes cares 
of resolving most of the system calls.

Mosix takes care of the communication between those 2 processes.
 </PARA> </SECT2> 
<SECT2><TITLE>The Mosix File System (MFS)</TITLE> <PARA>
MFS is a feature of openmosix which allows you to access remote filesystems in a cluster as if they were locally mounted.
The filesystem of your other nodes can be mounted on /mfs and you will e.g. find the files in /home on node 3 on each machine in /mfs/3/home

</PARA>
</SECT2>
<SECT2><TITLE>Direct File System Access (DFSA) </TITLE>
<PARA>
Both Mosix and 
openMosix provide a cluster-wide file-system (MFS) with the DFSA-option 
(direct file-system access). 
It provides access to all local and remote file-systems of the nodes in an 
Mosix or openMosix cluster.
</PARA>
</SECT2>

</SECT1>
<!--
<SECT1><TITLE>Work in Progress</TITLE>

<SECT2><TITLE>Network RAM</TITLE>
<PARA>

</PARA>
</SECT2>

<SECT2><TITLE>Migratable sockets</TITLE>
<PARA>

</PARA>
</SECT2>
<SECT2><TITLE>High availability</TITLE>
<PARA>

</PARA>
</SECT2>

</SECT1>
-->
</CHAPTER>

