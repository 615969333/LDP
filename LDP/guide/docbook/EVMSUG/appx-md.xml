<appendix id="appxmdreg"><title>The MD region manager</title>

<para>
Multiple disks (MD) support in Linux is a software implementation of RAID
(Redundant Array of Independent Disks).  The basic idea of software RAID
is to combine multiple inexpensive hard disks into an array of disks to
obtain performance, capacity, and reliability that exceeds that of a single
large disk.
</para>

<para>
Linux software RAID works on most block devices.  A Linux RAID device
can be composed of a mixture of IDE or SCSI devices.  Furthermore,
because a Linux RAID device is itself a block device, it can be a 
member of another Linux RAID device.
</para>

<para>
Whereas there are six standard types of RAID arrays (RAID-0 through RAID-5) in the hardware implementation, the Linux implementation of software
RAID has RAID-0, RAID-1, RAID-4, and RAID-5 levels.  In addition to these
four levels, Linux also has support for other non-redundant arrays called
"Linear Mode" and &quot;MULTIPATH.&quot;
</para>

<para>
All levels of Linux software RAID are discussed in greater detail in the
Software RAID HOWTO of The Linux Documentation Project 
(<ulink url="www.tldp.org">TLDP</ulink>).  
One important thing to remember is RAID is not a substitute for backups.
</para>

<sect1 id="createmdreg"><title>Creating an MD region</title>
<para>There are four EVMS MD region plug-ins: Linear, RAID-0, RAID-1,
and RAID-4/5.  The RAID-4/5 region plug-in provides support for both
RAID-4 and RAID-5 arrays.  After an MD region manager is selected, the software
provides a 
list of acceptable objects.  The ordering of the MD array is
implied by the order in which you pick objects from the provided list.  The
following are MD region configuration options:
</para>
<variablelist>
<varlistentry><term>chunk size</term>
<listitem><para>The smallest chunk size is 4 KB and the largest is 4096 KB.  The
chunk size is a power of 2 of the previous value.  Consider the intended use of the
MD region when selecting chunk size.  For example,
if the MD region contains mostly large files, you might see better performance
by having a larger chunk size.  The block size of the file system being used is
also an important factor when selecting chunk size.</para>
<para>This option is available for use with RAID-0 and RAID-4/5.</para></listitem>
</varlistentry>

<varlistentry><term>spare disk</term>
<listitem><para>The benefit of having a spare disk is that when an active disk fails,
the kernel MD code automatically replaces the failed disk with the spare disk.
Otherwise, the MD array operates in a degraded mode.</para>
<para>This option is available for use with RAID-1 and RAID-4/5.</para></listitem>
</varlistentry>

<varlistentry><term>RAID-5 algorithms</term>
<listitem><para>There are four RAID-5 parity algorithms: left asymmetric, right
asymmetric, left symmetric, and right symmetric.  The <ulink url="http://www.accs.com/p_and_p/RAID/LinuxRAID.html">ACCS</ulink> web
page provides examples of what the different parity algorithms do.</para>
<para>This option is available for use with the RAID-5 algorithm.</para></listitem>
</varlistentry>

</variablelist>

</sect1>

<sect1 id="addremspare"><title>Adding and removing a spare object (RAID-1 and RAID-4/5)</title>

<para>When adding a spare disk to an existing MD region, select an available object
that has the same size as the disks that are currently active in the MD region.
If the MD region consists of objects with different sizes, use the smallest size.</para>
<para>Note that after adding a spare to a degraded MD region, the kernel MD code
automatically starts the reconstruction of the MD array.  When reconstruction finishes,
the spare disks becomes an active disk.</para>
<para>If you want to reorganize disks and segments, you can remove an existing
spare disk from the MD region.  This is a safe operation because the spare disk does
not contain any data.</para>
</sect1>

<sect1 id="rmactiveraid"><title>Reconfiguring MD arrays</title>

<sect2><title>Expanding and shrinking MD arrays (linear and RAID-1)</title>
<para>If the MD region is part of a compatibility volume and the MD region is
the topmost object of the volume, it's possible to expand and shrink the MD region.</para>
<sect3><title>Expanding a linear MD region</title>
<para>A linear MD region can be expanded either by expanding the last member
or by adding a new member.</para>
</sect3>

<sect3><title>Shrinking a linear MD region</title>
<para>A linear MD region can be shrunk either by shrinking the last member
or by removing disks from the array (last member first).</para>
</sect3>

<sect3><title>Expanding a RAID-1 MD region</title>
<para>A RAID-1 MD region can only be expanded if all members can be expanded.</para>
</sect3>

<sect3><title>Shrinking a RAID-1 MD region</title>
<para>A RAID-1 MD region can only be shrunk if all members can be shrunk.</para>
</sect3>

</sect2>

<sect2><title>Adding an active object (RAID-1 only)</title>
<para>Use this option to increase the number of mirrors of the RAID-1 region, from
n-way mirrors to (n+1)-way mirrors.  When the operation is committed, the
kernel MD driver performs a resync of the MD array.</para>
</sect2>

<sect2><title>Removing an active object (RAID-1 only)</title>
<para>Use this option to decrease the number of mirrors of the RAID-1 region, from
n-way mirrors to (n-1)-way mirrors.  </para>
</sect2>
</sect1>


<sect1 id="rmfaultyobj"><title>Removing a faulty object (RAID-1 and RAID-4/5)</title>
<para>When an I/O error occurs on a disk, the disk is marked faulty by the kernel
MD driver.  Use this function to permanently remove the faulty disk from the MD region.</para>

</sect1>



<sect1 id="markobjfaulty"><title>Marking an object faulty (RAID-1 and RAID-4/5)</title>

<para>There are two scenarios for marking an active disk faulty:</para>

<itemizedlist>
<listitem><para>When the MD region has at least one spare disk, the active disk will
be swapped with a spare disk.</para></listitem>
</itemizedlist>
<itemizedlist>
<listitem><para>When the MD region has no spare disks, the active disk will be marked
faulty and the MD array will operate in degraded mode.</para></listitem>
</itemizedlist>

<para>When the active disk is successfully marked
faulty, it can be immediately removed from the MD region.  Alternatively, the
faulty disk can be later removed, as described in <xref linkend="rmfaultyobj"></xref>.</para>
</sect1>


<sect1 id="replaceobj"><title>Replacing an object</title>

<para>In EVMS 2.0 and later, you can replace a member of an MD region with an 
available storage object.  The new object must be the same size as the replaced object.
This option is currently only supported for volumes that are offline.</para>

</sect1>

<sect1 id="characsraid"><title>Characteristics of Linux RAID levels</title>

<para>The following subsections describe the characteristics of each Linux RAID level.</para>

<sect2><title>Linear mode</title>
<para>Characteristics:</para>
<itemizedlist>
<listitem>
<para>
Two or more disks are combined into one virtual MD device.</para>
</listitem>
<listitem>
<para>
The disks are appended to each other, so writing linearly to the MD device
fills up disk 0 first, then 1, and so on.</para>
</listitem>
<listitem>
<para>
The disks do not have to be of the same size.</para>
</listitem>
</itemizedlist>
<para>Advantages:</para>
<itemizedlist>
<listitem>
<para>
Can be used to build a very large MD device.
</para>
</listitem>
<listitem>
<para>
No parity calculation overhead is involved.
</para>
</listitem>
</itemizedlist>
<para>Disadvantages:</para>
<itemizedlist>
<listitem>
<para>
Not a "true" RAID because it is not fault-tolerant.
</para>
</listitem>
<listitem>
<para>
One disk crash will probably result in loss of most or all data.
</para>
</listitem>
<listitem>
<para>
Should never be used in mission-critical environments.</para>
</listitem>
</itemizedlist>
</sect2>

<sect2><title>RAID-0</title>

<para>Characteristics:</para>
<itemizedlist>
<listitem>
<para>
Two or more disks are combined into one virtual MD device.</para>
</listitem>
<listitem>
<para>
Also called "stripe" mode.
</para></listitem>
<listitem>
<para>
Stripe size determines how data is written to disk.  For example,
writing 16 K bytes to a RAID-0 array of three disks with stripe size
of 4 K bytes is broken down into:</para>
<itemizedlist>
<listitem>
<para>
4 K bytes of disk 0
</para>
</listitem>
<listitem>
<para>
4 K bytes to disk 1
</para>
</listitem>
<listitem>
<para>
4 K bytes to disk 2
</para>
</listitem>
<listitem>
<para>
4 K bytes to disk 0
</para>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<para>
The disks should be the same size but they do not have to be the same size.
</para>
</listitem>
</itemizedlist>

<para>Advantages:</para>
<itemizedlist>
<listitem>
<para>
Can be used to build a very large MD device.
</para>
</listitem>
<listitem>
<para>
I/O performance is greatly improved by spreading I/O load across many
controllers and disks.
</para>
</listitem>
<listitem>
<para>
No parity calculation overhead is involved.
</para>
</listitem>
</itemizedlist>

<para>Disadvantages:</para>
<itemizedlist>
<listitem>
<para>
Not a "true" RAID because it is not fault-tolerant.
</para>
</listitem>
<listitem>
<para>
One disk crash is liable to result in the loss of the whole array.
</para>
</listitem>
<listitem>
<para>
Should never be used in mission-critical environments.</para>
</listitem>
</itemizedlist>

</sect2>

<sect2><title>RAID-1</title>
<para>Characteristics:</para>

<itemizedlist>
<listitem>
<para>
Consists of two or more disks to provide a two-way or N-way mirrored MD device.
</para>
</listitem>
<listitem>
<para>
Writes result in writing identical data to all active disks in the array.
</para>
</listitem>
<listitem>
<para>
Reads can be performed on any active disk of the array.
</para>
</listitem>
<listitem>
<para>
Data is intact as long as there is at least one "good" active disk in the array.
</para>
</listitem>
<listitem>
<para>
The disks should be the same size.  If they are different sizes, the size
of the RAID-1 array is determined by the smallest disk.
</para>
</listitem>
</itemizedlist>

<para>Advantages:</para>
<itemizedlist>
<listitem>
<para>
100% redundancy of data.
</para>
</listitem>
<listitem>
<para>
Under certain circumstances, a RAID-1 array can sustain multiple simultaneous
disk failures.
</para>
</listitem>
<listitem>
<para>
Kernel MD code provides good read-balancing algorithm.
</para>
</listitem>
<listitem>
<para>
No parity calculation overhead is involved.
</para>
</listitem>
</itemizedlist>

<para>Disadvantages:</para>
<itemizedlist>
<listitem>
<para>
Write performance is often worse than on a single device.
</para>
</listitem>
</itemizedlist>
</sect2>

<sect2><title>RAID-4</title>
<para>Characteristics:</para>
<itemizedlist>
<listitem>
<para>
Consists of three or more striped disks.
</para>
</listitem>
<listitem>
<para>
Parity information is kept on one disk.  When a disk fails, parity information
is used to reconstruct all data.
</para>
</listitem>
<listitem>
<para>
The disks should be the same size.  If they are different sizes, the size of
the RAID-4 array is determined by the smallest disk.
</para>
</listitem>
</itemizedlist>

<para>Advantages:</para>
<itemizedlist>
<listitem>
<para>
Like RAID-0, I/O performance is greatly improved by spreading the I/O load
across many controllers and disks.
</para>
</listitem>
</itemizedlist>

<para>Disadvantages:</para>
<itemizedlist>
<listitem>
<para>
The parity disk becomes a bottleneck.  Therefore, a slow parity disk degrades
I/O performance of the whole array.
</para>
</listitem>
<listitem>
<para>
Cannot sustain a two-disk simultaneous failure.
</para>
</listitem>
</itemizedlist>
</sect2>

<sect2><title>RAID-5</title>

<para>Characteristics:</para>
<itemizedlist>
<listitem>
<para>
Consists of three or more striped disks.
</para>
</listitem>
<listitem>
<para>
Parity information is distributed evenly among the participating disks.
</para>
</listitem>
<listitem>
<para>
The disks should be the same size.  If they are different sizes, the size of
the RAID-5 array is determined by the smallest disk.
</para>
</listitem>
</itemizedlist>

<para>Advantages:</para>
<itemizedlist>
<listitem>
<para>
Like RAID-0, I/O performance is greatly improved by spreading the I/O load
across many controllers and disks.
</para>
</listitem>
<listitem>
<para>
Read performance is similar to RAID-0.
</para>
</listitem>
</itemizedlist>

<para>Disadvantages:</para>
<itemizedlist>
<listitem>
<para>
Writes can be expensive when required read-in blocks for parity calculations
are not in the cache.
</para>
</listitem>
<listitem>
<para>
Cannot sustain a two-disk simultaneous failure.
</para>
</listitem>
</itemizedlist>

</sect2>

<sect2><title>MULTIPATH</title>
<para>Characteristics:</para>
<itemizedlist>
<listitem><para>Consists of 1 or more disks.</para></listitem>
<listitem><para>Disks are actually I/O paths to the same physical disk.</para></listitem>
<listitem><para>Spreads I/O across active disks for simple load balancing.</para></listitem>
<listitem><para>Like other RAID levels, I/O failures will mark a disk faulty.</para></listitem>
<listitem><para>Failed I/O will be automatically retried on remaining active disks.</para></listitem>
</itemizedlist>

<para>Advantages:</para>
<itemizedlist>
<listitem><para>Achieves fault-tolerance through redundant I/O paths.</para></listitem>
<listitem><para>Possible performance improvements through load balancing.</para></listitem>
</itemizedlist>

<para>Disadvantages:</para>

<itemizedlist><listitem><para>Cannot survive a single disk crash.</para></listitem></itemizedlist>
</sect2>
</sect1>

	      
</appendix>

